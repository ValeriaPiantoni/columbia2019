{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://images-na.ssl-images-amazon.com/images/I/51ChIvQ9BCL.png width=200>\n",
    "\n",
    "For this drill, we are going to work from recalling simple string manipulation all the way through simple \"language processing\" -- that is, we will work from looking at text as a sequence of symbols all the way up to text as a collection of sentences which are made up of words that have semantic content. Our test case is a little light hearted, but has to do with the epithets the president assigns to people he does not particularly like. \n",
    "\n",
    "We all remember tweets like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Now that they realize the only Collusion with Russia was done by Crooked Hillary Clinton &amp; the Democrats, Nadler, Schiff and the Dem heads of the Committees have gone stone cold CRAZY. 81 letter sent to innocent people to harass them. They won’t get ANYTHING done for our Country!</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1102920338529288193?ref_src=twsrc%5Etfw\">March 5, 2019</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Now that they realize the only Collusion with Russia was done by Crooked Hillary Clinton &amp; the Democrats, Nadler, Schiff and the Dem heads of the Committees have gone stone cold CRAZY. 81 letter sent to innocent people to harass them. They won’t get ANYTHING done for our Country!</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1102920338529288193?ref_src=twsrc%5Etfw\">March 5, 2019</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Crooked\" is part of a simple pattern the president followed on the campaign trail and continues even just this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Wacky Nut Job <a href=\"https://twitter.com/AnnCoulter?ref_src=twsrc%5Etfw\">@AnnCoulter</a>, who still hasn’t figured out that, despite all odds and an entire Democrat Party of Far Left Radicals against me (not to mention certain Republicans who are sadly unwilling to fight), I am winning on the Border. Major sections of Wall are being built...</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1104503216111321089?ref_src=twsrc%5Etfw\">March 9, 2019</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Wacky Nut Job <a href=\"https://twitter.com/AnnCoulter?ref_src=twsrc%5Etfw\">@AnnCoulter</a>, who still hasn’t figured out that, despite all odds and an entire Democrat Party of Far Left Radicals against me (not to mention certain Republicans who are sadly unwilling to fight), I am winning on the Border. Major sections of Wall are being built...</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1104503216111321089?ref_src=twsrc%5Etfw\">March 9, 2019</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A list of insults](https://en.wikipedia.org/wiki/List_of_nicknames_used_by_Donald_Trump) has been compiled at Wikipedia. You can take a look. We can start by reading these in and seeing if there is something we can extract. Recall that in addition to `read_csv()` we also have `read_html()` in the Pandas package. The latter function will read all the tables on a web page and assign each to a new element in a list. \n",
    "\n",
    "Here we just read in the insults broken out into separte tables for politicians, world leaders and so on. First we import the function we need and then we read the data from the Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_html\n",
    "insults = read_html(\"https://en.wikipedia.org/wiki/List_of_nicknames_used_by_Donald_Trump\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `header=0` because the first row (index 0) represents the labels of our columns. So how many tables did we get? And let's look at the first couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(insults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insults[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment**\n",
    "\n",
    "Let's see what kind of insult each of these represents. A few years ago, a group at the University of Vermont decided they would judge the \"happiness\" of social media over time by looking at how \"happy\" each tweet or facebook post was. They scored the text using a happiness dictionary where each of 10,000 words was assigned a numerical score (0 meant depressed, 10 meant giggly). The paper was called [The Geography of Happiness](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) and you should have a look. \n",
    "\n",
    "While they went on to score entire sentences or blog posts, we're going to just try single words. The data were published by the authors as a CSV. The data are on our GitHub page and you can either [download them here](https://github.com/computationaljournalism/columbia2019/raw/master/data/happy.csv) and place them in the folder with this notebook, or you can read the CSV directly from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "happy = read_csv(\"https://github.com/computationaljournalism/columbia2019/raw/master/data/happy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Give me the five happiest scoring words and the five lowest scoring words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the score for a few of the single-word epithets used by the president. Maybe \"Crooked\" or \"Lyin'\". Some words (especially those that replace \"ing\" with \"in'\") might not be in the Vermont data set, so use the proper spelling if you like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at one of the old tweets referring to \"Crooked Hillary\". We are going to create a single string called `tweet` to hold the text. Use a string method (and if it has been too long, you can refer to the [string documentation](https://docs.python.org/3/library/stdtypes.html#string-methods)) to find the number of the character (counting characters from the beginning of the string) that represents the start and end of the word \"Crooked\". To be clear, the word \"that\" in the tweet below begins with index 4 and ends with index 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Now that they realize the only Collusion with Russia was done by Crooked Hillary Clinton &amp; the Democrats, Nadler, Schiff and the Dem heads of the Committees have gone stone cold CRAZY. 81 letter sent to innocent people to harass them. They won’t get ANYTHING done for our Country!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the beginning and end of the word \"Crooked\" and use subsetting to pull out the word.** (Remember how you can subset a string using square braces and a colon.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lots of tweets**\n",
    "\n",
    "Now, let's look at the President's tweets from the last 12 months. As we did for the Regular Expression drill, [download a CSV from our GitHub](https://github.com/computationaljournalism/columbia2019/raw/master/data/trump_tweets_year.txt) and put it in the same folder as this notebook. (I have edited the original file because it had a mistake. Please download it again.) Now, let's read it in. Remember that `set_option()` will let us control the number of characters shown in each cell so we can see all 280 for a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, set_option\n",
    "\n",
    "# prepare cells to show 280 characters\n",
    "set_option(\"display.max_colwidth\",1000)\n",
    "\n",
    "rdt = read_csv(\"trump_tweets_year.csv\")\n",
    "rdt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Use the `.str.contains()` method we learned about last time (in notebook 10b) on the `text` column to create a series of `True` and `False` values, one for each tweet -- `True` if the tweet contains \"Crooked\" and `False` otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Do the same thing with \"Crooked Hillary\" -- did he use \"Crooked\" to describe anyone else?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a small data set that contains tweets with the name \"Hillary\" in them that is preceeded by a word. Have a look at the regular expression and tell me what it's doing. Use regexper.com if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary = rdt[rdt[\"text\"].str.contains(r\".*\\b[a-z]+ Hillary.*\",case=False)]\n",
    "hillary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to introduce a group using the parentheses and apply the `.str.replace()` command we learned in notebook 10b to replace the string with just the word before \"Hillary\". Again, use regexper.com to see what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary[\"text\"].str.replace(r\".*\\b([a-z]+) Hillary.*\",r\"\\1\",case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. To this column, use `.value_counts()` to tablulate the different words that came before \"Hillary\" in his tweets. Anything stand out or is it just an assortment of typical words and not one of his insults?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Try this with another single-word insult and another politician.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A step farther in language processing**\n",
    "\n",
    "So far we have been dealing with text as patterns of symbols -- letters, numbers, punctuation. The next level up in the chain is to think about languge itself. This is a corner of machine learning called Natural Language Processing or NLP. There is so much to say about this, but in short, it is a suite of methods and theories for automatically parsing text, recognizing words and classifying them by part of speech or by function in a sentence (subject, object, etc.). \n",
    "\n",
    "NLP then moves forward from these base pieces to perform operations like classifying text or building other kinds of models. We are going to use a package called `spaCy` that makes some of this analysis easy. First we install the package and then we have to download a \"model\". This consists of the components of vocabulary and the computational means to perform the tagging and other operations we alluded to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a new NLP engine that we'll just choose to call `nlp`. With this object we can take a text string and start to parse out the different words, identifying their part of speech and grouping them into \"entities\". We'll use as our text a tweet from the president about Hillary again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up spacy and then create an english language parser called nlp()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parser nlp() to parse a tweet from the president.\n",
    "\n",
    "tweet = \"The only Collusion with the Russians was with Crooked Hillary Clinton and the Democratic National Committee...And where’s the Server that the DNC refused to give to the FBI? Where are the new Texts between Agent Lisa Page and her Agent lover Peter S? We want them now!\"\n",
    "document = nlp(tweet)\n",
    "\n",
    "type(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have are a list of tokens, each one essentially being a word in the tweet from the president. Let's see what kinds of things each token contains. Since `document` is a list-like object, we can create a loop and print out some data for each token. We start with just the token and it's estimated part of speech. We access the latter with a `.pos_` data attribute for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in document:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pull out how the various entities in the text. These are groups of tokens that represent people or places or organizations. They are stored in the `.ents` attribute of the `document` object and are again essentially a list of entities. Each entity has data consisting of the text (written description) and a label that says what kind of thing it is. The list of labels [is given here](https://spacy.io/api/annotation) and just for reference `NORP` stands for \"Nationalities or religious or political groups.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in document.ents:\n",
    "    print(ent.text, \"-\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without going too far into the details yet, we can now run through the tokens, pulling out proper nouns (`PROPN`) and then all the words that have a \"compound\" relation with the word -- meaning its part of a multiword expression. so \"Agent Lisa Page\" in the text above is an exmple. \n",
    "\n",
    "Below we pull out (crudely) the proper nouns for which there are associated multiword expressions. The hope would be that we could use this to build a parser that would automatically find expressions like \"Crooked Hillary\" or \"Wacky Nut Job Ann Coulter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in document:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        comps = [j for j in token.children if j.dep_ == \"compound\"]\n",
    "        if comps:\n",
    "            print(\" \".join([c.text for c in comps]),token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point here is to show you the range of things that are possible as we dive into text. From simple string manipulations, perhaps comparing words to a dictionary; to regular expressions; to proper natural language processing. There's some great tools! If I've lit a fire, there is an excellent [\"spaCy 101\" tutorial](https://spacy.io/usage/spacy-101) that you can how follow along with. There are so many questions that are now answerable by you about text we've seen in class. You are unstoppable!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
