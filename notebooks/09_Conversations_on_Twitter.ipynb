{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/chatbots-talking.jpg width=500>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Conversations on Twitter ###\n",
    "\n",
    "Today we are going to talk about the texture of conversation happening on sites like Twitter. With talk of efforts to promote civility and organic discussion, many of your projects are attempting to understand the flow of information between accounts on Twitter, say. We are going to have a look at the various techniques we might use to assess that or make it visible. This is just and introduction and we can have much more to say as the semester progresses.\n",
    "\n",
    "**Cascade from the NYT**\n",
    "\n",
    "In an attempt to understand the conversation around its articles, the New York Times performed an experiment in the \"golden age\" of Twitter data access (remember Gnip?). The Times merged data from three sources -- its web access logs (We saw those from `digg.com` when Mike showed us how accesses to their web site was recorded, item by item, line by line.) They also had logs from `bit.ly` which is the URL shortener behind `nyti.ms`, watching people shorten URLs. Finally, with Twitter data, they could see references to their articles float across the network -- all the mentions, favorites, retweets and replies.  \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/data%20docs%201.001.jpeg width=500>\n",
    "<br><br>\n",
    "**Lesson \\#1 of data science — data are promiscuous!** These three data sets can be \"linked.\" The web access logs owned by the Times include your IP address as part of the browsing transaction. Ah and `bit.ly` also has your IP address when you use their API to shorten a URL. [(Um, yes, `bit.ly` has an API!](https://dev.bitly.com/v4_documentation.html). So browsing and shortening can be linked. Then, we can watch Twitter for the appearance of your shortened link. This is a timing thing and connections are looser, but still \"good enough for government work\" as we used to say at NASA — I mean in a joking way.\n",
    "\n",
    "We can then watch `bit.ly` for when people click their links, expand a URL and land at an article on `nytimes.com`. Associating the particular tweet with a `bit.ly` link used to be a piece of math. ([Which we even patented!](http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=9,135,576.PN.&OS=PN/9,135,576&RS=PN/9,135,576) Remember that blissful time when software patents weren't pure evil? Oh and these days CMS's generate unique URLs making this process less math-fussy.) And, for a final flourish, we can watch the person who clicked on your (presumably your) tweet and see what kind of browsing they do on the Times' site.\n",
    "\n",
    "Spooky, right? \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/data%20docs%201.002.jpeg width=500>\n",
    "<br><br>\n",
    "OK so that's the whole transaction. And so we build. For each article appearing in the Times, we could track their activity on Twitter. What networks get activated? Who retweets whom? We found beautiful quiet networks... like the group of rabbis that shared each religion story and commented amongst themselves. The simple tools of who shares what with whom, who is talking to whom, but seen over time — these provide us with great investigative angles for stories. To help, we wrapped it up with a lovely graphical interface. \n",
    "\n",
    "**Lesson \\#2 of data science — data need a face!** There is no natural look to data and when we have a huge system like Twitter, we need help bringing them down to an understandable scale. [Lev Manovich calls this the anti-sublime.](http://manovich.net/index.php/projects/data-visualisation-as-new-abstraction-and-anti-sublime) Anyway, that's enough of the digital humanities for one day. Here's what Jer Thorp came up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "VimeoVideo(54858819,width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what we can do**\n",
    "\n",
    "First, we are going to have a look at conversations that mention `@AOC`. Our first look will be through a platform called [Hoaxy](https://hoaxy.iuni.iu.edu). It's a platform not a programming approach which means you might outgrow it quickly, but we will show you how to create similar graphics from scratch. Let's have a look at a [\"network diagram\"](https://hoaxy.iuni.iu.edu/#query=%40AOC&sort=recent&type=Twitter) that maps out who is mentioning, retweeting, replying to whom.\n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/aoc.jpg width=500 style=\"border:1px solid black\">\n",
    "<br><br>\n",
    "Hoaxy produces a network diagram, where each point represents an account on Twitter. The connections represent two accounts \"in communication\" by retweeting, replying or mentioning one another (with the relationship being described as *directional* -- I might mention you, but you might not ever mention me). The colors of the points depict a scale that is meant to describe the \"bot like\"ness of an account. \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b1.jpg width=500 style=\"border:1px solid black\">\n",
    "<br><br>\n",
    "Here is another exmple, this time a [network diagram for \"Trey Gowdy\".](https://hoaxy.iuni.iu.edu/#query=Trey%20Gowdy&sort=recent&type=Twitter) We can have a look at the discussion here, and see small conversations happening in very botty contexts. Oh and he comes up because of this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">“If thinking that James Comey is not a good FBI Director is tantamount to being an agent of Russia, than just list all the people that are agents of Russia - Chuck Schumer, Nancy Pelosi, Rod Rosenstein who wrote the memo to get rid of Comey, the Inspector General....” Trey Gowdy</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1098204256602066945?ref_src=twsrc%5Etfw\">February 20, 2019</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">“If thinking that James Comey is not a good FBI Director is tantamount to being an agent of Russia, than just list all the people that are agents of Russia - Chuck Schumer, Nancy Pelosi, Rod Rosenstein who wrote the memo to get rid of Comey, the Inspector General....” Trey Gowdy</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/1098204256602066945?ref_src=twsrc%5Etfw\">February 20, 2019</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the network. And there is so much to dig into... so many different pieces of \"information\" floating around and entering in and out of the conversation. Play with it a little. Explore, read, change.\n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/h1.jpg width=500 style=\"border:1px solid black\">\n",
    "<br><br>\n",
    "**Bot, bot, bot, goose!**\n",
    "\n",
    "The good people at Indiana Univeristy have \"learned\" characteristics of bot behavior, producing a score for any account. The learning (which we will return to formally later in the term) involves contrasting data from known bot accounts and \"real\" accounts. Each Twitter account is reduced to a number of characteristics to make this comparison. The list below was taken from one of their academic papers. In general, this is a hard learning problem.\n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b8.jpg width=400 style=\"border:1px solid black\">\n",
    "<br><br>\n",
    "In general, this is a hard learning problem. But the color scale might give you a rough indication of when the conversation taking place involves authentic or somehow immitation accounts. And we might want to know the difference because it will help us judge whether we are witnessing (or partaking in) a genuine exchange of opinions or are instead part of an amplification campaign of some kind. Propaganda.\n",
    "\n",
    "**Lesson \\#3 of data science — Never trust an algorithm you didn't write, and even then be very cautious!** We can use these scores in various ways, always checking to see if they make sense, of course. Here we look at Trey Gowdy's followers\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/tg1.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "...  and the people he follows.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/tg2.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "**For you — Ten minutes**\n",
    "\n",
    "From the BotoMeter interface, request a group of your followers and export the data (there is a button at the bottom of the page). Download it and bring it into your notebook. Then, compute the median content score of this collection of your followers. Ready? Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group at Indiana has made their bot-or-not scaling available via an API complete with a [Python interface](https://github.com/IUNetSci/botometer-python)! You can install the `botometer` and then use your Twitter credentials and a key from RapidAPI. You will need a key to use the API -- you are limited to 2,000 calls per day. You can apply [here](https://rapidapi.com/OSoMe/api/botometer?utm_source=mashape&utm_medium=301) (RapidAPI is an API hosting service of sorts.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install botometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab your keys from a previous notebook or https://apps.twitter.com\n",
    "\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "# here's my key - i'm not sure it will work for you or if it's tied to my twitter account\n",
    "MashKey = \"NAeSF7TTbymshxelRHhCXqlQVfc0p1zgYQojsnVombDJddPvas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's kick the tires on this! We can look up one person or a group. Let's try a few..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botometer import Botometer\n",
    "\n",
    "meter = Botometer(wait_on_ratelimit=True,\n",
    "                  mashape_key=MashKey,\n",
    "                  consumer_key = CONSUMER_KEY,\n",
    "                  consumer_secret = CONSUMER_SECRET,\n",
    "                  access_token = ACCESS_TOKEN,\n",
    "                  access_token_secret = ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a single account by screen name\n",
    "result = meter.check_account('@emilybell')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = meter.check_account('@MatthewAlbasi')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one more -- James-ugh... [https://twitter.com/JamesKe54983151](https://twitter.com/JamesKe54983151)\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b9.jpg width=500 style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = meter.check_account('@JamesKe54983151')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This person seems so earnest! And might be genuine but his assets have certainly made their way around the web....\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b7.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "Sometimes you win, sometimes you lose. \n",
    "\n",
    "**Network analysis, building our own**\n",
    "\n",
    "Let's now have a look at a conversation and make a network diagram on our own. This is a useful exercise in that we now have complete control over what features of an account we choose to highlight and what kinds of connections we want to establish. Let's setup the Twitter API using the `Cursor` to pull a number of tweets on a topic -- I picked `#nationalemergency` but today `Trey Gowdy` is awesome too. \n",
    "\n",
    "So, let's set up the `API` object and then make a call to the `Cursor` using a list comprehension from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API, Cursor\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `Cursor` to actually pull our 1,000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = [status for status in Cursor(api.search,q='#nationalemergency', result_type='recent').items(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `pprint()` to have a look at Tweepy's \"raw\" tweet object, fashioned from just the built-in objects like numbers and strings and lists and dictionaries. Here's the first tweet object from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(searches[0]._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so now we are going to compile the information we need using the basic Python object that is returned in the `.json` object from Tweepy. Let's create a data frame that has one row per \"connection\" -- someone mentioning someone else. \n",
    "\n",
    "This is a moment where other forms of iteration break down and we have to use a loop. So, well, here we go! We are going to create a \"dictionary of lists\" form of a data frame, where we have one list for the \"froms\" (the account doing the tweeting), one for the \"to's\" (the account mentioned), with the \"edges\" indicating the relationship we have, the \"mention.\" \n",
    "\n",
    "Have a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to make a dictionary of lists, where each list is a column in the table\n",
    "\n",
    "froms = []\n",
    "tos = []\n",
    "edges = []\n",
    "\n",
    "# iterate through the 1000 tweet objects we collected\n",
    "for tweetobj in searches:\n",
    "\n",
    "    # use the built-in python form of the data (dictionaries and lists)\n",
    "    tweet = tweetobj._json\n",
    "    \n",
    "    # pull the screen name of the \"from\"\n",
    "    screen_name = tweet[\"user\"][\"screen_name\"]\n",
    "    \n",
    "    # pull the \"to's\" that are mentions (assuming accounts are mentioned)\n",
    "    if \"user_mentions\" in tweet[\"entities\"]:\n",
    "        \n",
    "        for mention in tweet[\"entities\"][\"user_mentions\"]:\n",
    "            \n",
    "            # pull out the name of the \"to\"\n",
    "            mention_name = mention[\"screen_name\"]\n",
    "            \n",
    "            # and add to the lists\n",
    "            edges.append(\"mention\")\n",
    "            tos.append(mention_name)\n",
    "            froms.append(screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine these three columns into the format needed We will import this file to [Graph Commons, a handy, interactive and shareable tool for making network graphs.](https://graphcommons.com) The platform looks for information on nodes (Twitter accounts) and edges (mentions or retweets or whatever). They can be uploaded as CSV files and technically you just need to specify the edges. \n",
    "\n",
    "Here is a sample edge CSV file from the Graph Commons web site.\n",
    "\n",
    ">`FromType, FromName, Edge, ToType, ToName\n",
    "Person, John Doe, FRIENDS, Person, Emily\n",
    "Person, John Doe, MEMBER OF, Organization, Acme`\n",
    "\n",
    "Make an account and let's make a CSV to upload. Oh, and Graph Commons is cool because many people can work on one graph at the same time, adding new data in pieces, fashioning a bigger graph. It's like Google Docs for network diagrams. Yes, lovely.\n",
    "\n",
    "So, let's first make a data frame that has the five columns given in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame({\"FromType\":\"Twitter\",\"FromName\":froms,\"Edge\":edges,\"ToType\":\"Twitter\",\"ToName\":tos})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see who was mentioned the most often. This is a simple `.value_counts()` on the `ToName` column. Ah, Pandas, is there nothing you can't do? 🤷‍♀️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ToName\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And maybe put our bot API to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = meter.check_account(\"GrrrGraphics\")\n",
    "print(result[\"display_scores\"][\"english\"])\n",
    "\n",
    "result = meter.check_account(\"JaySekulow\")\n",
    "print(result[\"display_scores\"][\"english\"])\n",
    "\n",
    "result = meter.check_account(\"PatriotPennsy\")\n",
    "print(result[\"display_scores\"][\"english\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, not to be complicated, but if someone tweets multiple times and mentions the same account each time, the \"strength\" of the relationship should be greater than if an account only mentions another just once. So we are going to do a no-no and use something a bit advanced to create a dataframe that removed all the duplicate rows (people mentioning others multiple times) and adding a `Weight` column that represents the number of such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's output this to a CSV and read into a new Graph Commons diagram. Go ahead, give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a moment and switch over to Graph Commons, uploading the `test.csv` file. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/gc.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "**For you!**\n",
    "\n",
    "Now, pick a new topic and have a look at who's involved. Tell me a story of who's mentioning whom? Now, there are so many different relationships, the code below repeats the process but this time allows for mentions, retweets and replies. We will let our `edge` list contain the name of each kind of interaction. Otherwise all else is the same, including the mysterious line that sums things up and provides row weights.\n",
    "\n",
    "OK, pick a new topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = [status for status in Cursor(api.search,q='#nationalemergency', result_type='recent').items(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... make the data frame and output the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froms = []\n",
    "tos = []\n",
    "edges = []\n",
    "\n",
    "# iterate through the 1000 tweets we collected\n",
    "for tweetobj in searches:\n",
    "\n",
    "    # use the built-in python form of the data (dictionaries and lists)\n",
    "    tweet = tweetobj._json\n",
    "    \n",
    "    # pull the screen name of the \"from\"\n",
    "    screen_name = tweet[\"user\"][\"screen_name\"]\n",
    "    link = []\n",
    "\n",
    "    # the \"link\" list will hold the names of people we have included in our \n",
    "    # graph already (so if someone retweets and mentions someone, we only get one of these)\n",
    "    \n",
    "    # pull the \"to's\" that are retweets\n",
    "    if \"retweeted_status\" in tweet:        \n",
    "        retweet_name = tweet[\"retweeted_status\"][\"user\"][\"screen_name\"]\n",
    "        link.append(retweet_name)\n",
    "        \n",
    "        edges.append(\"retweet\")\n",
    "        tos.append(retweet_name)\n",
    "        froms.append(screen_name)\n",
    "    \n",
    "    # pull the \"to's\" that are replies\n",
    "    if tweet[\"in_reply_to_screen_name\"]:\n",
    "        reply_name = tweet[\"in_reply_to_screen_name\"]\n",
    "        \n",
    "        if not reply_name in link:\n",
    "             \n",
    "             edges.append(\"reply\")\n",
    "             tos.append(reply_name)\n",
    "             froms.append(screen_name)\n",
    "            \n",
    "    # pull the \"to's\" that are mentions\n",
    "    if \"user_mentions\" in tweet[\"entities\"]:\n",
    "        \n",
    "        for mention in tweet[\"entities\"][\"user_mentions\"]:\n",
    "            mention_name = mention[\"screen_name\"]\n",
    "            \n",
    "            if not mention_name in link:\n",
    "                link.append(mention_name)\n",
    "                \n",
    "                edges.append(\"mention\")\n",
    "                tos.append(mention_name)\n",
    "                froms.append(screen_name)\n",
    "\n",
    "# make a data frame\n",
    "df = DataFrame({\"FromType\":\"Twitter\",\"FromName\":froms,\"Edge\":edges,\"ToType\":\"Twitter\",\"ToName\":tos})\n",
    "\n",
    "# remove and count duplicate rows\n",
    "df = df.groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "\n",
    "# output to a CSV\n",
    "df.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All about (or a lot about... or some about) bots! ###\n",
    "\n",
    "Today we wanted to give you a little overview of the kinds of bots that are circulating in the world, providing a taxonomy, if you will, of all the innovative work being done. But first, what is a bot? \n",
    "\n",
    "We'll draw mostly from [botnerds.com](http://botnerds.com) and the [article by Mark Sample](https://medium.com/@samplereality/a-protest-bot-is-a-bot-so-specific-you-cant-mistake-it-for-bullshit-90fe10b7fbaa#.ikymcdf6q). The former reference offers a practical definition...\n",
    ">Bots are software programs that perform automated, repetitive, pre-defined tasks.  These tasks can include almost any interaction with software that has an API.\n",
    "\n",
    "...while Mark Sample is more aspirational.\n",
    ">A computer program that reveals the injustice and inequality of the world and imagines alternatives. A computer program that says who’s to praise and who’s to blame. A computer program that questions how, when, who and why. A computer program whose indictments are so specific you can’t mistake them for bullshit. A computer program that does all this automatically.\n",
    "\n",
    "And then of course, the State of California has defined a bot in it's recently passed bot legislation.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/u5.001.jpeg>\n",
    "\n",
    "So let's dig in a bit and send you off with a bot homework!\n",
    "\n",
    "![Bot](http://21clradio.com/wp-content/uploads/2016/02/Kiddle-Logo.png)\n",
    "\n",
    "First, the practical. Our first reference divides bots into \"Good\" and \"Bad\" categories, which are interpreted largely in terms of their effect on our information ecosystem -- functionally, adding or detracting.\n",
    "\n",
    "1. Good Bots\n",
    "    * **Chatbots** \"are designed to carry on conversations with humans, usually just for fun, and to test the limits of the technology.\"\n",
    "    * **Crawlers** \"run continuously in the background, primarily fetch data from other APIs or websites, and are *well-behaved* in that they respect directives you give them\"\n",
    "    * **Transactional bots** act as agents on behalf of humans, and interact with external systems to accomplish a specific transaction, moving data from one platform to another.\"\n",
    "    * **Informational bots** \"surface helpful information, often as push notifications, and are also called *news bots*.\"\n",
    "    * **Art bots** \"are designed to be appreciated aesthetically.\"\n",
    "    * **Game bots** \"game bots function as characters, often for humans to play against or to practice and develop skills...\"\n",
    "2. Bad Bots\n",
    "    * **Hackers** \"distribute malware of all kinds\"\n",
    "    * **Spammers** \"steal content (email addresses, images, text, etc) from other website\", often to republish it\n",
    "    * **Scrapers** post \"promotional content around the web, and ultimately drive traffic to the spammer’s website\"\n",
    "    * **Impersonators** \"mimic natural user characteristics, making them hard to identify\" (they cite [political propadanda bots](http://www.businessinsider.com/political-bots-by-governments-around-the-world-2015-12/#mexico-1))\n",
    "    \n",
    "In terms of Bot Agency or Bot Intelligence, this framing presents examples along a spectrum -- Script Bots, Smart Bots and Intelligent Agents. An interaction with a Script Bots, they write, is\n",
    "\n",
    ">based off of a pre-determined model (the “script”) that determines what the bot can and cannot do.  The “script” is a decision tree where responding to one question takes you down a specific path, which opens up a new, pre-determined set of possibilities. \n",
    "\n",
    "Upping the autonomy a little, Smart Bots have access to other APIs that expand the universe of responses.\n",
    "\n",
    ">Many bots have a heavy server-side processing component, which allows them access to massive computing power in understanding and responding to queries.  Couple that with the open-sourcing of AI software libraries like Theano and TensorFlow, and you have the ingredients for some amazing human-bot interactions.\n",
    "\n",
    "This category also allows for human-assisted interactions. The bot need not act alone, but can invoke human intelligence or even direct consultation, redirecting the interaction to a responsible human. Finally, the Intelligent Agent is meant to act autonomously.\n",
    "\n",
    "> If operating correctly, they should require no human intervention in order to perform their tasks correctly.  Google’s self-driving cars are designed without steering wheels for humans, because they shouldn’t be necessary.  x.ai has a bot that schedules meetings for you, Amy Ingram, and she manages all the back-and-forth with zero oversight.\n",
    "\n",
    "Mark Sample provides a different, less practical characterization of bots, one drawn more from literary studies (where bots have been an object of fascination for some time). He focuses on one particular kind of bot (primarily active on Twitter) that he terms \"Protest Bots\" or \"Bots of Conviction\". Sample says they share at least five characteristics: \n",
    "\n",
    "* **Topical** - \"They are about the morning news — and the daily horrors that fail to make it into the news.\"\n",
    "* **Data-based** - \"They don’t make this [stuff] up. They draw from research, statistics, spreadsheets, databases.\" \n",
    "* **Cumulative** - It is the nature of bots to do the same thing over and over again, with only slight variation...  The repetition builds on itself, the bot relentlessly riffing on its theme, unyielding and overwhelming, a pile-up of wreckage on our screens.\"\n",
    "* **Oppositional** - \"Bots of conviction challenge us to consider our own complicity in the wrongs of the world.\"\n",
    "* **Uncanny.** - \"Protests bots often reveal something that was hidden; or conversely, they might purposefully obscure something that had been in plain sight.\"\n",
    "\n",
    "The examples of Protest Bots that Sample introduces are often journalistic, but often more in the realm of advocacy. Still, the examples open up the potential to the kinds of projects or actions that can be taken outside the more functional description of our first reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">The next person to be executed in Ohio is Cleveland Jackson in 3 months, 8 days, 22 hours and 13 minutes. <a href=\"https://t.co/uD8HuiZIfM\">https://t.co/uD8HuiZIfM</a></p>&mdash; The Next To Die (@thenexttodie) <a href=\"https://twitter.com/thenexttodie/status/1098247685386264576?ref_src=twsrc%5Etfw\">February 20, 2019</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">The next person to be executed in Ohio is Cleveland Jackson in 3 months, 8 days, 22 hours and 13 minutes. <a href=\"https://t.co/uD8HuiZIfM\">https://t.co/uD8HuiZIfM</a></p>&mdash; The Next To Die (@thenexttodie) <a href=\"https://twitter.com/thenexttodie/status/1098247685386264576?ref_src=twsrc%5Etfw\">February 20, 2019</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weekend, you will be imaginging and making your own bots and we hope this outline has helped prime the pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Bot Account & App\n",
    "\n",
    "**1) Create a New Twitter User for Your Bot**\n",
    "\n",
    "Before we get started, you'll want to create a new Twitter account for your bot! It's best to not create a bot out of your personal Twitter account, but I will leave that up to you!\n",
    "\n",
    "Go to [https://twitter.com/signup](https://twitter.com/signup) (you'll have to log out of your normal account or go incognito) and create a new account. **You will need to use your phone number when siging up** or you wont be able to create a new Twitter app in next step. You can sign up for a [https://voice.google.com/](Google Voice number) if you don't want to use your own phone number, or if Twitter gives you a hard time for having too many accounts tied to a single phone number.\n",
    "\n",
    "\n",
    "**2) Create a New Twitter App for Your Bot**\n",
    "\n",
    "You are a pro at this! Once you have created your new Twitter account, create a new Twitter app for your bot.\n",
    "\n",
    "1. Go to [https://apps.twitter.com](https://apps.twitter.com/) and log in with your new Twitter user account.\n",
    "2. Click “Create New App”\n",
    "3. Fill out the form, agree to the terms, and click “Create your Twitter application”\n",
    "4. Click on “Keys and Access Tokens” tab, and copy your “API key” and “API secret”. Scroll down and click “Create my access token”, and copy your “Access token” and “Access token secret”.\n",
    "\n",
    "Once you have your tokens, copy them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert your own keys and secrets here...or just use Mark's! he won't mind\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And maybe print a little \"It's alive\" message..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call the \"me\" api to make sure you using the Twitter api as your bot\n",
    "print('Ok, we are ready to tweet as ' + api.me().screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Send a Tweet From Our Bot!\n",
    "\n",
    "We will be using the `statuses/update` api to send the tweet: https://dev.twitter.com/rest/reference/post/statuses/update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your bot's first tweet\n",
    "api.update_status(status='Whe the people...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Great! Now let's take a quick detour....\n",
    "\n",
    "In a few minutes, we are going to create a simple \"news\" bot: a bot that tweets out the latest stories from The New York Times. But, how are we going to get the NYTimes latest stories? Let's turn to [RSS](https://en.wikipedia.org/wiki/RSS).\n",
    "\n",
    "Most news sites on the internet publish an RSS feed. Here are a few:\n",
    "\n",
    "[New York Times \"HomePage\"](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "[Wired](https://www.wired.com/feed/)\n",
    "\n",
    "[WNYC Radio Lab Podcast](http://feeds.wnyc.org/radiolab)\n",
    "\n",
    "To use RSS feeds in our code, we're going to use the python module called [Feedparser](https://pypi.python.org/pypi/feedparser). `Feedparser` does the hard work of fetching and parsing the feeds for us. RSS feeds can be very messy and this module does an amazing job of dealing with the mess and handing us a nice python object (`dictionary`) to work with! \n",
    "\n",
    "Let's install the module and start working with some RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to install the `feedparser` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a short example of how we can fetch the RSS feed for The New York Times \"HomePage\" stories.\n",
    "\n",
    "The \"HomePage\" RSS feed can be found here: [http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "The code below uses the `feedparser` module to fetch the RSS feed (remember HTTP requests?), parse it and return it as a python dictionary. This module does the hard work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's fetch the New York Times Homepage RSS Feed\n",
    "from feedparser import parse\n",
    "\n",
    "# the URL of the homepage stories RSS feed\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# fetch the RSS feed and parse it\n",
    "feed = parse(nytimes_rss_url)\n",
    "\n",
    "# what type of object are we dealing with?\n",
    "print(type(feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, the function `parse` will return a dictionary-like object, meaning we store our data under `keys()`. Let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feed.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's have a closer look at the `feed` information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed['feed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of thing do we have? What kind of data do we have? Now, lets look at the `entities`, which is a list of the stories found in the feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, let's print out the stories (titles and urls) in the RSS feed\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a break from \"news\" and look at the RSS feed for [Atlas Obscura](http://www.atlasobscura.com/) (a lovely site about travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, Let's Get Back to Our Bot!\n",
    "\n",
    "If we wanted to tweet out the latest story from Atlas Obscura, we combine our Twitter and our RSS/feedparser examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler, API\n",
    "from feedparser import parse\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)\n",
    "\n",
    "# now, get the Atlas Obscura feed\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "# let's take only the 1st story in our list\n",
    "first_story = feed['entries'][0]\n",
    "\n",
    "# now, create the text of the tweet using the story title and link/url\n",
    "tweet_text = 'This is really interesting! ' + first_story['title'] + ' ' + first_story['link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at what we're about to tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, tweet it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api.update_status(status=tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: Let's Clone the @twoheadlines Bot\n",
    "\n",
    "[Darius Kazemi](https://twitter.com/tinysubversions) created a clever bot called [@twoheadlines](https://twitter.com/twoheadlines) where he combines two different headlines in to a single tweet:\n",
    "\n",
    "> Comedy is when you take two headlines about different things and then confuse them\n",
    "\n",
    "Let's do a simple clone of the `@twoheadlines` bot by combining the first half of a New York Times headline with the second half of a Breitbart headline :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "# fetch the nytimes and breitbart RSS feeds\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "breitbart_rss_url = 'http://feeds.feedburner.com/breitbart'\n",
    "\n",
    "nytimes_feed = parse(nytimes_rss_url)\n",
    "breitbart_feed = parse(breitbart_rss_url)\n",
    "\n",
    "# get the first story from each of the two feeds\n",
    "nytimes_first_story = nytimes_feed['entries'][0]\n",
    "breitbart_first_story = breitbart_feed['entries'][0]\n",
    "\n",
    "print('nyt: ',nytimes_first_story['title'],\"\\n\")\n",
    "print('breit: ',breitbart_first_story['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the two headlines into a single headline\n",
    "nytimes_words = nytimes_first_story['title'].split(' ')\n",
    "breitbart_words = breitbart_first_story['title'].split(' ')\n",
    "\n",
    "# take the 1st half of the nytimes \"words\" plus the second half of the breitbart \"words\n",
    "new_words = nytimes_words[:len(nytimes_words)//2] +\\\n",
    "            breitbart_words[len(breitbart_words)//2:]\n",
    "\n",
    "# this is python weirdness to take a list of words\n",
    "# and join them together with a space between each word\n",
    "new_headline = ' '.join(new_words)\n",
    "print(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your bot can tweet the combined headline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api.update_status(status=new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that's cute but how can we create a long-running bot?\n",
    "\n",
    "Everything we've done up to now just runs once and then exits/stops. Let's look at how we can have something run forever - our bot doesn't need to sleep much!\n",
    "\n",
    "Python has a great [`time`](https://docs.python.org/2/library/time.html), which handles various time-related functions (duh!). The `time` module also has a very helpful method called `sleep()`, which tells our program to sleep, or \"pause\", for a number of seconds. Let's take a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop 10 times, pausing for 1 second during each iteration\n",
    "for number in range(0, 10):\n",
    "    print(number)\n",
    "    \n",
    "    # sleep for one second\n",
    "    sleep(1)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a simple \"forever\" loop to get our script to run until we stop it. The code below will loop forever, pausing for 1 second, until you hit the stop button in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    print('hello')\n",
    "    \n",
    "    # sleep for one second\n",
    "    time.sleep(1)\n",
    "    \n",
    "# to get this to stop, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's put it all together and build our news bot\n",
    "\n",
    "This is a very simple \"news\" bot, which will tweet out new top stories from The New York Times. The bot will check the NYTimes HomePage RSS feed every 10 seconds - if it sees a new story, it will tweet it.\n",
    "\n",
    "I'm also adding some super complicated AI, to add some color-commentary to each story that our bot tweets.\n",
    "\n",
    "This code uses a new module called [`random`](https://docs.python.org/2/library/random.html), which makes it easy to randomly select an item from a `list`.\n",
    "\n",
    "*So you don't put extra stress on The New York Times servers, you should sleep every 60 seconds (at least). We are only sleeping for 10 seconds here for demo purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this \"bot\" will tweet out any new stories published in the nytimes homepage\n",
    "from time import sleep    \n",
    "from feedparser import parse\n",
    "from random import choice\n",
    "\n",
    "insightful_things_to_say = [\n",
    "    'this is really interesting',\n",
    "    'great read -->',\n",
    "    'hmmm....',\n",
    "    'amazing',\n",
    "    'how does this happen?',\n",
    "]\n",
    "\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# keep track of the previous nytimes link/url that we tweeted\n",
    "prev_tweeted_link = ''\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    \n",
    "    # fetch and parse the NYTimes RSS feed\n",
    "    nytimes_feed = parse(nytimes_rss_url)\n",
    "\n",
    "    # get the first story\n",
    "    first_story = nytimes_feed['entries'][0]\n",
    "\n",
    "    # take the link of the first story and see if we've tweeted it before\n",
    "    link = first_story['link']\n",
    "    if link != prev_tweeted_link:\n",
    "        # it's new, lets tweet it out!\n",
    "        print('new story - lets tweet it: ' + link)\n",
    "  \n",
    "        # build the text of our tweet\n",
    "        tweet_text = choice(insightful_things_to_say) + ' ' + first_story['title'] + ' ' + first_story['link']\n",
    "        \n",
    "        # fire it off to twitter\n",
    "        api.update_status(status=tweet_text)\n",
    "        \n",
    "        # keep track of the this link that we just tweeted\n",
    "        prev_tweeted_link = link\n",
    "    else:\n",
    "        # we've already tweeted this...no new stories\n",
    "        # nothing to do\n",
    "        print('no new story...lets wait a little while')\n",
    "\n",
    "    # sleep for a little while\n",
    "    sleep(10)\n",
    "\n",
    "    \n",
    "# if you want to stop this script, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework, you will create a bot that responds to data in realtime, perhaps retweeting another account..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I&#39;m a bot that retweets <a href=\"https://twitter.com/realDonaldTrump?ref_src=twsrc%5Etfw\">@realDonaldTrump</a> so you don&#39;t have to follow him. <a href=\"https://twitter.com/hashtag/unfollowtrump?src=hash&amp;ref_src=twsrc%5Etfw\">#unfollowtrump</a></p>&mdash; I Retweet Trump (@IRetweetTrump) <a href=\"https://twitter.com/IRetweetTrump/status/782440751372251136?ref_src=twsrc%5Etfw\">October 2, 2016</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I&#39;m a bot that retweets <a href=\"https://twitter.com/realDonaldTrump?ref_src=twsrc%5Etfw\">@realDonaldTrump</a> so you don&#39;t have to follow him. <a href=\"https://twitter.com/hashtag/unfollowtrump?src=hash&amp;ref_src=twsrc%5Etfw\">#unfollowtrump</a></p>&mdash; I Retweet Trump (@IRetweetTrump) <a href=\"https://twitter.com/IRetweetTrump/status/782440751372251136?ref_src=twsrc%5Etfw\">October 2, 2016</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or perhaps look at what goes missing. [Politwoops](https://projects.propublica.org/politwoops/), now supported by ProPublica is a good example. \n",
    "\n",
    "## Some computing tools for programming with \"language\"\n",
    "\n",
    "The Twitter bot Mike prepared relies on mashing up two headlines. Some of that might get better if we knew a little about what the headline described. What is the subject? What action is described? Some of these questions are addressed by a field of computer science (well, computational linguistics) called Natural Language Processing. There are plenty of tools in Python for making use of the fruits of this research. \n",
    "\n",
    "We will be using a package called [TextBlob](https://textblob.readthedocs.io/en/dev/) that is a simplified version of the Natural Language Toolkit in Python. (Sometimes tools become really powerful for practitioners and leave non-experts behind. That's what has happened, to some extent, with the NLTK. It's a little hard to just \"jump in\". And so TextBlob is like computational training wheels.) [Allison Parrish's Natural Language Basics with TextBlob](http://rwet.decontextualize.com/book/textblob/) is a great place to read about what TextBlob is good for. \n",
    "\n",
    "First, we need to install the package. Off to PIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('brown')\n",
    "download('punkt')\n",
    "download('maxent_ne_chunker')\n",
    "download('words')\n",
    "download('conll2000')\n",
    "download('maxent_treebank_pos_tagger')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the package for this session and bring in a headline from todays New York Times. We read it in as a string but preface the quotes with a \"u\". That tells Python the string is in Unicode -- publishers use fancy quotation marks, for example, that are not the simple \" or '. \n",
    "\n",
    "The TextBlob() function takes text and turns it into a \"TextBlob\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "headline = u\"For Millennial Investors, a Harsh Lesson in Market Gyrations\"\n",
    "tb = TextBlob(headline)\n",
    "\n",
    "type(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextBlob object has a number of attribures that have processed the text. The simplest are lists of words and sentences. Here we pull just the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously a better approach than the one we took when we just split a string on spaces -- a technique that didn't handle punctuation like commas and periods well. OK that's a good trick but there are better ones! For example, TextBlob's language processing let's it estimate which words are part of noun phrases. \n",
    "\n",
    "There are various techniques for doing this and none of them are perfect. To be fair, using a headline means using a text fragment and not a sentence. The language processing tools are usually trained on full sentences of text. Still, it's not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun phrases are obtained by extracting information from a \"tagged\" version of the text. Here the tags represent parts of speech. You can see [a complete list of the tags here.](https://cs.nyu.edu/grishman/jet/guide/PennPOS.html) The parts of speech are stored as a list of word-tag pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(tb.tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .tags attribute is a list. (See the square brackets?) The list elements are a new data type called a \"tuple\" which is like a list, for our purposes. So you can take, say the first element of the tags list and look at the first and second elements of the tuple (the word and its estimates part of speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I'm not wild about it, TextBlob also provides an estimate of the sentiment of the statement. That is, is the text expressing a positive or negative sentiment. I'll leave you to consult the Parrish blog post or the TextBlob documentation of this lovely feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same thing to a different headline. Mashing them up might mean replacing one noun phrase with another. How might you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline2 = u\"Fake News Flashback: CNN Declared Trump ‘Bonkers’ for Saying Clinton, Dems Behind Dossier\"\n",
    "tb2 = TextBlob(headline2)\n",
    "tb2.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb2.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing. There are various methods to \"parse\" text -- different algorithms for tagging words in a sentence, for extracting noun phrases and for estimating sentiment. You can replace the default when you call TextBlob. The documentation describes other noun phrase extractors. Here's how you would use the ConllExtractor, based on a data set compiled for the Conference on Computational Natural Language Learning (CoNLL-2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob.np_extractors import ConllExtractor\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "tb = TextBlob(headline,np_extractor=extractor)\n",
    "tb.noun_phrases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
