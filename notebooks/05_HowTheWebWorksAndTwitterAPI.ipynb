{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Today we launch into the basics of how the web works -- how data shuttles around the internet. We wanted to start the day, however, with two loose ends. So far we have seen entries into stories where investigative techniques involved some kind of computation. The Axios publication on the President's schedule produced [a nice interactive piece](https://www.axios.com/donald-trump-private-schedules-leak-executive-time-34e67fbb-3af6-48df-aefb-52e02c334255.html) that's worth reading and thinking what you might ask next.\n",
    "<br><br>\n",
    "Next, we want to spend a little time with the \"Learn to Code\" meme. Erin and others have done a little work with our collection of 100K tweets. Let's see where they're at and maybe suggest where to go next!*\n",
    "\n",
    "\n",
    "\n",
    "# How The Web Works... and an Introduction to the Twitter API\n",
    "<br>\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/1920/1*CWytxLBZtxrxekPofi0-RQ.png width=500>\n",
    "<br>\n",
    "\n",
    "Before we take take a look at APIs, let's take a step back and learn about how the web works. Understanding some of the fundamentals about how information travels around the internet will help us a ton. And, the best place to start is by learning about HTTP, the Hypertext Transfer Protocol.\n",
    "\n",
    "## HTTP\n",
    "\n",
    "A simple protocol called [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) powers most of the communications on the web, including your browser and probably most of the apps that you use. HTTP allows you (via your browser, a mobile app or even code you write!) to **request** data (HTML or a web page, PDFs, MP3s, etc.) from a service across the internet (e.g. google.com, twitter.com) and that service will respond with the requested data (i.e. the **response**). \n",
    "\n",
    "Let's take a look at how HTTP in more detail by looking at Mike's slides on \"How the Internet Works.\" You can find the [Keynote](https://github.com/computationaljournalism/columbia2019/blob/master/docs/HowTheInternetWorks.key) (for Mac's) or [PDF](https://github.com/computationaljournalism/columbia2019/blob/master/docs/HowTheInternetWorks.pdf) copy of the slides in the [`docs` directory](https://github.com/computationaljournalism/columbia2019/tree/master/docs) of our github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Review of HTTP: Request and Response\n",
    "\n",
    "So, let's review a simple HTTP request:\n",
    "\n",
    "1. A \"**client**\" (your browser, your Instagram app, or even some code that you are about to write) makes a **request** for data.\n",
    "\n",
    "2. The \"**request**\" is in the form of a [URL](https://en.wikipedia.org/wiki/URL) (Uniform Resource Locator -- a web address). The URL specifies the site you are requesting information from and the page/document/data you want. For example: https://nytimes.com/ is the site for the New York Times and this URL https://www.nytimes.com/interactive/2018/12/10/business/location-data-privacy-apps.html specifies a given news story in the form of an HTML page.\n",
    "\n",
    "3. The \"**server**\" receives the request and then returns the page/document/data you asked for. This is the **response**.\n",
    "\n",
    "Simple!\n",
    "\n",
    "One important note: this type of request is called a \"**GET**\" request. There are other types of HTTP requests which we'll learn about later. (The main difference being in how you specify the data you want -- GET specifies the data you want in the URL of the request as we'll see below.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a URL\n",
    "\n",
    "The [URL](https://en.wikipedia.org/wiki/URL), or Uniform Resource Locator, or \"web address,\" contains a variety of important information about data that we are requesting. Here are the various fields in a URL:\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/43bd353c3d0879547481da33bba7d15768bdf4bb/68747470733a2f2f7261772e6769746875622e636f6d2f41544c2d5744492d437572726963756c756d2f686f772d7468652d696e7465726e65742d776f726b732f6d61737465722f696d616765732f616e61746f6d792d75726c2e706e67\" width=500>\n",
    "    \n",
    "For now, we're just going to focus on the protocol, domain and path. The parameters are very important but we'll come back to that in a future lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Kind of Data is on the Other End of a Request?\n",
    "\n",
    "The data you find in a web page (HTML) or PDF document is meant to be read as you would read the page of a book. But in this class, we'll learn that that kind of reading is labor-intensive. We want a computer to read for us instead -- to take in the data and create something new. This means we want other formats, which lead us to CSV, JSON and XML.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[State of the Union, and specifically recent TV Ratings (**HTML**)](https://en.wikipedia.org/wiki/State_of_the_Union):\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`https://en.wikipedia.org/wiki/State_of_the_Union`](https://en.wikipedia.org/wiki/State_of_the_Union)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[NOAA Daily Weather Records (**HTML**)](https://www.ncdc.noaa.gov/cdo-web/datatools/records):  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`https://www.ncdc.noaa.gov/cdo-web/datatools/records`](https://www.ncdc.noaa.gov/cdo-web/datatools/records)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[USDA Expenditures on Children by Families (**PDF**)](https://catalog.data.gov/dataset/expenditures-on-children-by-families):  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`https://www.cnpp.usda.gov/sites/default/files/expenditures_on_children_by_families/crc2013.pdf`](https://www.cnpp.usda.gov/sites/default/files/expenditures_on_children_by_families/crc2013.pdf)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[FDNY Monthly Response Times (**CSV**)](https://data.cityofnewyork.us/Social-Services/FDNY-Monthly-Response-Times/j34j-vqvt):  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`https://data.cityofnewyork.us/api/views/j34j-vqvt/rows.csv`](https://data.cityofnewyork.us/api/views/j34j-vqvt/rows.csv)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[FDNY Monthly Response Times (**JSON**)](https://data.cityofnewyork.us/Social-Services/FDNY-Monthly-Response-Times/j34j-vqvt):  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`https://data.cityofnewyork.us/resource/6b8a-2fci.json`](https://data.cityofnewyork.us/resource/6b8a-2fci.json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough About URLs! Let's Write Some Code\n",
    "\n",
    "Ok, time for us to write some code to make out own HTTP requests. There are many python libraries which handle all of the fun of HTTP for us - we'll use one simply called [`requests`](http://docs.python-requests.org/en/master/).\n",
    "\n",
    "To install the `requests` Python library, you can run the following. Recall that the double percent signs indicate that the code in the cell is to be interpreted as something other than Python commands. In this case, we are giving instructions to the UNIX **sh**ell. (For those of you on Windows, replace the \"%%sh\" with \"%%cmd\" as you want the following instruction interpreted as something you'd type at the command prompt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will make an HTTP request to `http://digg.com` a popular news aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "# Specify the location of the information you want as a string\n",
    "\n",
    "url = 'http://digg.com'\n",
    "\n",
    "# Then fetch the data (the resource) at that address using get() from\n",
    "# the \"requests\" package\n",
    "\n",
    "response = get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is `response`? Remember that we can inspect the object to see what type it is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure how much that helps us so let's jump over to the [`requests` library documentation](http://docs.python-requests.org/en/master/) to see how we use this library. In particular, we can look at the [Response documentation](http://docs.python-requests.org/en/master/api/#requests.Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out the HTTP status code\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also print out the \"headers\" sent back by the digg.com server\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best of all, we can see the page we've requested (the digg.com homepage in this case) using the following code?\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This is the same as opening the URL in Chrome and selecting `View ➡️ Developer ➡️ View Source`\n",
    "\n",
    "### Before moving on...\n",
    "\n",
    "What do you think happens on the other end of our HTTP request to `digg.com`? Well, we know that a server receives our **request** and then passes back the information that we've asked for (the digg homepage in this case). The `digg.com` servers also keep a record of each requests in \"log\" files. Let's take a quick peek at the digg log files.\n",
    "<br><br>\n",
    "\n",
    "<img src=http://1.bp.blogspot.com/-TmWAXEghGtc/U20Nrs7sYvI/AAAAAAAAAXg/niP4Hf1Ef4U/s1600/Gilliam+Intermission.jpg width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web logs, likely started as a way for \"web masters\" to assess the health of their servers, to make sure that the service was performing properly and to keep an eye on its usage. Toward that end, web servers (the program we just saw operating at digg.com) standardized on information they record with each request, [the so-called \"common log format.\"](https://en.wikipedia.org/wiki/Common_Log_Format)\n",
    "\n",
    ">1st column — The remote host (ie the client IP)<br>\n",
    "2nd column — The identity of the user determined by identd (not usually used since not reliable)<br>\n",
    "3rd column — The user name determined by HTTP authentication.<br>\n",
    "4th column — The time the request was received.<br>\n",
    "5th column — The request line from the client. (“GET / HTTP/1.0”)<br>\n",
    "6th column — The status code sent from the server to the client (200, 404 etc.)<br>\n",
    "7th column — The size of the response to the client (in bytes)<br>\n",
    "8th column — Referer is the page that linked to this URL.<br>\n",
    "9th column — User-agent is the browser identification string.<br>\n",
    "\n",
    "As you saw, each request generates a new line in the log file, and your individual requests are threaded in time with data from everyone else's use of the site at the same time. Web site owners quickly realized that these logs not only tell you information about broken links and busy or quiet times for your service, *they also can give you a snapshot of user's interests.*\n",
    "\n",
    "How might we use these data to tell us something about visitors to our web site? Perhaps, specifically, what might digg.com learn about it's users news consumption habits?\n",
    "\n",
    "By pulling out the lines related to just your requests to digg.com, for example, we can start to see patterns in what you are searching for. What are your interests? What are your habits? How does your use of the service compare to others? Are there \"clusters\" or groups of people with similar behaviors to yours? All of these questions can be answered by log analysis. [Wired had a nice overview of log analysis](https://www.wired.com/2010/02/gather_users_data_from_server_logs/) that's worth a look.\n",
    "\n",
    "Our point here, though, is to give you a complete look, end-to-end, of an HTTP request and the traces that are left behind.\n",
    "\n",
    "### A Quick Exercise\n",
    "\n",
    "Write some code in the box below to make an HTTP request to The New York Times homepage. After you make the request, print the homepage HTML. Ready? Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up Question**: Have a look at the HTML page. Some of you will have had experience writing or reading HTML (which hopefully makes you appreciate Markdown). You see `tags` that are used to structure the information on the page. You might she headers `<h1>` say, or paragraphs `<p>` or anchor tags (links) `<a>`. \n",
    "\n",
    "Now, try to find all the headlines on the page. Do they have special tags? Is there other information in the tags that indicate the content is a headline? When we surf the web making requests from Python, we have to learn to *parse* HTML pages and figure out how to pull out the information we are interested in. digg.com might be interested in headlines, for example. You might be interested in stories listed under the *Technology* section. So, looking at the NYTimes homepage HTML, what patterns do you noite? \n",
    "\n",
    "Code which fetches a page, like the NYTimes, and parses out the headlines is an example of \"web scraping.\"\n",
    "\n",
    "This isn't to say that it's all difficult. There is one kind of tag that is especially easy to work with. It's the `<table>` tag. It, well, creates a table. Have a look at the [Wikipedia page about the State of the Union Address](https://en.wikipedia.org/wiki/State_of_the_Union). It has a table of TV Ratings. Just so we don't forget about Pandas, we can pull that table into a DataFrame and start to work with it. We have seen `read_csv()` for the Axios data, but now we'll try `read_html()` for a `<table>` on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_html\n",
    "\n",
    "wiki_sotu = read_html(\"https://en.wikipedia.org/wiki/State_of_the_Union\")\n",
    "print(type(wiki_sotu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list! One element per table on the page. Use the `View ➡️ Developer ➡️ View Source` to see that there are other `<table>` tags floating around to structure the different parts of the page. How many objects in the list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(wiki_sotu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some code to pull out the first table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... now the second..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now the fourth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the headers for the table are included in the first row of the table. That's too bad. We can tell `read_html()` to use the first row of the table (index 0) for the headings of the columns. The *argument* is called `header`. (We are also going to specify that \"TBD\" is a missing value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the HTMl page, collecting the tables\n",
    "wiki_sotu = read_html(\"https://en.wikipedia.org/wiki/State_of_the_Union\",header=0,na_values=\"TBD\")\n",
    "\n",
    "# Pull out the fourth that has the TV ratings, call the DataFrame \"ratings\"\n",
    "ratings = wiki_sotu[3]\n",
    "\n",
    "# And then have a look!\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's a little \"mean\"ingless, take the average of the total viewers column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings[\"Viewers, millions\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When someone puts the data you want on a web page, putting it in a table is a huge advantage. It is probably the closest thing that you can get to \"publishing data\" on a web page. There are still issues, like figuring out which table on the page or if everything is formatted properly, but it can be pretty easy. \n",
    "\n",
    "There is a lot more to learn about HTTP and \"web scraping\" but we'll pick that up in future lessons. For now, let's move on to APIs! With an API we get data, honest to goodness data, and not some piece of a document made to look sorta like data.\n",
    "\n",
    "## What's an API?\n",
    "\n",
    "An API, or application programming interface, allows you to specify the data you want and returns it in a computer-friendly format like [JSON](https://www.json.org/) or [XML](https://en.wikipedia.org/wiki/XML) rather than HTML. The \"interface\" is a regularized way to make requests, and a consistent specification for the data you asked for. So many organizations now publish APIs for their data. From [The New York Times](https://developer.nytimes.com/) to [ProPublica](https://propublica.github.io/campaign-finance-api-docs/), to governmental organizations like the ~~[EPA](https://developer.epa.gov/category/api/)~~, to social media sites like [Twitter](https://developer.twitter.com/en/docs) and [Instagram](https://www.instagram.com/developer/) and [LinkedIn](https://developer.linkedin.com).\n",
    "\n",
    "**The idea of an API is quite old,** and in fact APIs exist throughout the operating system in your computer. There is an API that lets different applications on your computer access printing capabilities, or communicate via your wireless hardware. These APIs, again, provide application developers with a regularized way to access services. So Word's print screen looks like the print screen from your PDF previewer or even Photoshop.\n",
    "\n",
    "**Then in time, the services that were being advertised moved from your computer to the web.** So-called \"mashups\" came on the scene that let you feed data from one service into another. To put this in a vague historical perspective, if Web 1.0 meant putting your content online, then Web 2.0 was about cooperation between sites, sharing data via the internet to build new services. \n",
    "\n",
    "Salesforce.com led the way with its API in 2000 (I believe), recognizing that customers needed the same data across different platforms. Ebay followed, providing an API so that others could embed their data and services. Personally, it was the Google Maps' API that really drove the idea home. It appeared in 2006 and immediately spawned a number of mapping mashups. You can read about the history of APIs from [a services perspective](https://history.apievangelist.com/), [as evolution of the mashup](https://www.ibm.com/developerworks/library/x-mashups/index.html), or as [a technical innovation](http://www.openlegacy.com/blog/the-history-of-apis-and-how-they-impact-your-future), eventually leading back to a [PhD thesis in 2000 by Roy Fieldings](http://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm) laying out the whole scheme. \n",
    "\n",
    "Today there are so many APIs it's hard to keep track. Look at the growth, captured by the \"readmeblog\".\n",
    "\n",
    "<img src=https://blog.readme.io/content/images/2016/11/Screenshot-2016-11-01-16.01.29.png width=500>\n",
    "\n",
    "Ah, but fortunately someone is keeping track for us! Have a look at [ProgrammableWeb](https://www.programmableweb.com/) for all the latest APIs. They have a [report on API growth](https://www.programmableweb.com/news/programmableweb-api-directory-eclipses-17000-api-economy-continues-surge/research/2017/03/13) that includes a table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api_tables = read_html(\"https://www.programmableweb.com/news/programmableweb-api-directory-eclipses-17000-api-economy-continues-surge/research/2017/03/13\")\n",
    "api_tables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK now we're just showing off. \n",
    "\n",
    "**Google's autocomplete**. Let's start our API fun by looking at the API that powers Google's auto-complete/suggest feature. Everytime you start typing something into your Chrome browser or the Google search box, it will make suggestions for you. That's all negotiated by API. You can [read about it here](https://shreyaschand.com/blog/2013/01/03/google-autocomplete-api/). Notice that this is an \"unpublished\" API in that its specifics are not documented by Google. \n",
    "\n",
    "Here is how you'd access that programmatically (that is, with Python and not with a browser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a request -- here it is like we've typed \"donald trump is\"\n",
    "url = \"http://suggestqueries.google.com/complete/search?client=firefox&q=donald trump is\"\n",
    "response = get(url)\n",
    "\n",
    "# print out the response we get -- we aren't print()ing it so you can see it's a string\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude - JSON\n",
    "\n",
    "The `response.text` is a string. But it's in a format that looks eerily familiar. If you had to guess what would you say this string represents? That is, if you were to read the string as a piece of Python, what kinds of data do you see? What structures is it organized in?\n",
    "\n",
    "The output here looks like a list of lists. It is formatted in something called the JavaScript Object Notation and you can [read about it here.](https://www.json.org/) or see [a tutorial here.](https://restfulapi.net/introduction-to-json/) (just read up to the \"syntax\" part). organizing data in JSON looks a lot like wht we do in Python (with some small exceptions). \n",
    "\n",
    "So in the string above, we see square braces that mean a \"list\" in Python or an \"array\" in JSON. A \"dictionary\" in Python is specified in the same way as an \"object\" in JSON. There are some subtle differences like `None` in Python is `null` in JSON. But let's ignore that for the moment. \n",
    "\n",
    "So what we see is Google providing us data in a format we can use directly in our code. The `requests` library returns an object that not only has the `.text` of the response, but it also parses it into a Python object, storing it in an attribute called `.json`. \n",
    "\n",
    "Let's make our request again - this time we will work with the Python object that's created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://suggestqueries.google.com/complete/search?client=firefox&q=donald trump is\"\n",
    "response = get(url)\n",
    "\n",
    "# the requests library helps us with JSON. here, we can convert the response (JSON) to a python object\n",
    "data = response.json()\n",
    "\n",
    "# what type of object is it?\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write some code to pull out the fifth suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to auto-complete another phrase and look at the results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Technical note.* A URL can't include spaces, but the `requests` package and your browser are now smart enough clean things up before they send it to Google or whatever service you're pulling data from. So adding \"donald trump is\" as the query string is strictly speaking not right, but the environment is making up for the mistake. We'll say more about character encodings later.\n",
    "\n",
    "### API Authentication\n",
    "\n",
    "The autocomplete from Google is designed to be used widely. We simply made a request and received data. Most API providers, on the other hand, require you as the developer to use a form of authentication while using their APIs. This way they know who's doing what and can impose limits (so that you don't put too much of a drain on their servers, say). There are various forms of authentication: OAuth, API keys and even username and passwords.\n",
    "\n",
    "For example, like [The New York Times](https://developer.nytimes.com/) only require that you use an API key when making API calls. With API keys, you usually just pass the key in your API calls, like:\n",
    "\n",
    "```\n",
    "https://developer.nytimes.com/article_search_v2.json?api_key=abcxyz&q=tesla\n",
    "```\n",
    "\n",
    "[OAuth](https://en.wikipedia.org/wiki/OAuth) is a bit more complicated but provides more fine-grained control for the API service as well as the users. Let's come back to it right after we set up our Twitter API keys (yep, they use OAuth for their API authentication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using The Twitter API\n",
    "\n",
    "To access the Twitter API, we need to register an \"application\" that will be pulling data from their service. This means that in one sweet instant you have become a developer! 😮 The steps are pretty easy and listed below. You'll first need to get a set of \"keys\" to use the API and then install a Python library that exposes the Twitter API through special objects. You are well on your way to writing your very own mis-information bot! (kidding)\n",
    "    \n",
    "**A) Apply for a Twitter Developer Account**\n",
    "\n",
    "If you don't already have credentials for Twitter, you have to create apply for a developer account, create an application and then generate a set of keys (an API key, API secret, Access token and Access token secret) on the Twitter developer site. First, we need to apply for an account:\n",
    "\n",
    "1. Create a Twitter user account if you do not already have one.\n",
    "2. Go to [https://developer.twitter.com/](https://developer.twitter.com/) and log in with your Twitter user account. \n",
    "3. After you've logged in, you need to \"Apply\" for a developer account. You can do that here: [https://developer.twitter.com/en/apply-for-access](https://developer.twitter.com/en/apply-for-access). A few tips when Applying for your developer account:\n",
    "\n",
    "  * On the \"Account Details\" section, you can select \"I am requesting access for my own personal use.\"\n",
    "  * On the \"Tell us about your project\" section, you can select: \"Student project / Learning to code.\" In the \"Describe in your own words what you are building\" section, let them know you'll be using the API in class. Do you best to fill this part out and let us know if you need help!\n",
    "  * The last step is to confirm your email. Once you do this, your application will be \"under review\" by the Twitter team\n",
    "\n",
    "**Important**: At this point, you may have to wait for Twitter to review and approve your account. In some cases, your application will be approved immediately. Once you are approved, follow the next steps to create an application and generate your API \"keys.\"\n",
    "\n",
    "\n",
    "**B) Get Your API Keys**\n",
    "\n",
    "Once your application is approved:\n",
    "\n",
    "1. Go to [https://developer.twitter.com/en/apps](https://developer.twitter.com/en/apps) and click “Create an app\"\n",
    "2. Fill out the form, agree to the terms, and click the \"create\" button. When filling out the form, make sure you put information in the required fields. For URL, you can use `https://columbia.edu`\n",
    "3. Once you've created your app, click on “Keys and tokens” tab and copy your “API key” and “API secret key”. Scroll down to the \"Access token & access token secret\" and click \"create\" - copy your “Access token” and “Access token secret”.\n",
    "\n",
    "Once you have your tokens, copy them into the variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = ''  # API key\n",
    "consumer_secret = ''  # API secrect key\n",
    "access_token = ''  # access token\n",
    "access_token_secret = ''  # access token secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Install the Tweepy Library**\n",
    "\n",
    "The developer community has created [hundreds of Twitter libraries](https://developer.twitter.com/en/docs/developer-utilities/twitter-libraries) that help you access Twitter's API. By \"help\" we mean they have created objects that hide the details of making requests for data from Twitter, and leave you with a clean coding interface. Your requests to Twitter are in the form of neat methods (verbs) that return data on users, their statuses and followers. You can even post tweets using these libraries.\n",
    "\n",
    "We will by using Tweepy to call the Twitter API. Why? It has many of the best features of the other libraries and its documentation is complete. Often, free software projects can be thinly documented, leaving you a little out to sea if you have a problem.\n",
    "\n",
    "Keep these two links open in tabs as we go through the code below: [Tweepy documentation](https://tweepy.readthedocs.io/en/3.7.0/\n",
    ") and [source code](https://github.com/tweepy/tweepy).\n",
    "\n",
    "Use the following to install the Tweepy library on your machine. (Again, if you are on a Windows machine, you replace \"%%sh\" with \"%%cmd\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)\n",
    "print(type(api))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has methods that encapsulate all the functionlity of the Twitter API. You can follow or unfollow people, you can post tweets, you can pull an accounts follower or friends. All of these are \"methods\" of the `api` object. It exposes all the activities available through the Twitter API.\n",
    "\n",
    "So from the comfort of Python we can make extensive use of Twitter.\n",
    "\n",
    "\n",
    "### Getting a User's Profile Info\n",
    "\n",
    "OK, now you are prepped and ready to start making Twitter API calls. First, lets look at some user profiles. \n",
    "\n",
    "We will be calling the `users/show` API. [It is documented by Twitter here.](https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-users-show)\n",
    "\n",
    "To call the `user/show` API, Tweepy has a corresponding method called [`get_user()`](http://tweepy.readthedocs.io/en/v3.6.0/api.html#user-methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a user's profile ('myoung' in this case)\n",
    "user = api.get_user('myoung')\n",
    "\n",
    "type(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of information do we get about a user? Take a look at the `user/show` documentation to see what Twitter returns. By calling `api.get_user()` above, the Tweepy library made an API call to Twitter. The result of the API call is technically a JSON string. We could parse it into primitive Python objects like lists and dictionaries and numbers and strings. \n",
    "\n",
    "Tweepy does this work for us and stores it in an attribute `._json`. Let's have a look at all the goodies we can learn about `myoung`. (Again, we are not `print()`ing this object because, like DataFrames, the notebook is going to format the output nicely.)\n",
    "\n",
    "What kind of object are we dealing with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = user._json\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the data Python object, pull some information about Mike from this dictionary.\n",
    "\n",
    "1. What is his `location`? \n",
    "2. What his his `description`? \n",
    "3. When did he last tweet? \n",
    "4. What was the text of his tweet?\n",
    "5. How many followers does he have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy also uses the high-level `User` object returned by  `.get_user()` that results from the API call to store this information in a more convenient format -- as attributes of the object. Here we have `.id` and `.screen_name` and the date Mike joined Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's print out a few attributes for our user\n",
    "print(user.id)\n",
    "print(user.screen_name)\n",
    "print(user.location)\n",
    "print(user.followers_count)\n",
    "print(user.listed_count)\n",
    "print(user.statuses_count)\n",
    "print(user.created_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so to sum. Twitter has an API. We make requests and get back a JSON string. Packages like `Tweepy` work with that string. We've seen a simple translation into Python objects using the `._json` attribute and we've seen the object `Tweepy` creates that has special attributes for characteristics of a user. Basically, making an object that is easier to work with than the raw dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try This!** \n",
    "\n",
    "Modify the code above to get the user profile information for your own account, and then `@realDonaldTrump`, and then one other person of your choosing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a Tweet!\n",
    "\n",
    "If you were writing a bot, it would need to tweet! You can send out a tweet with one line of code.\n",
    "\n",
    "We will be using the `statuses/update` API to send the tweet. Again, like all of Twitters API functionality, it has documentation and you can [read about posting status updates here.]( https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/post-statuses-update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# send a tweet!\n",
    "# you may want to modify this message before you send it\n",
    "\n",
    "api.update_status(status='Sending my first tweet via the Twitter API - I love Computational Journalism!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, through the API, you can do anything through Python that you could do via your Twitter interface. So, you can tweet, you can make friends, you can send a direct message, you can read your direct messages... the works!\n",
    "\n",
    "### Look at a User's Tweets\n",
    "\n",
    "Now, let's look at [`@realDonaldTrump`](https://twitter.com/realDonaldTrump)'s tweets. If you've had enough of all that, replace it with [`@justinbieber`](https://twitter.com/justinbieber) or someone less anxious-making.\n",
    "\n",
    "We will use the `statuses/user_timeline` API to do this. [Read the documentation here.](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html) Essentially it returns tweets in reverse chronological order -- newest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the realDonaldTrump last 10 tweets\n",
    "tweets = api.user_timeline(screen_name='realDonaldTrump', count=10)\n",
    "\n",
    "# what sort of object do we get?\n",
    "print(type(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class here is a `ResultSet`. It's just like a list. So you can ask how many tweets we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or ask for the sixth one (remember we start counting at 0). Just like the `user` object we made, let's look at one of his tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet = tweets[5]\n",
    "type(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK a `Status` object. Like the `User` object, we can pull raw data corresponding to Trump's sixth tweet using the `._json` attribute of this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can pull out `tweet[\"created_at\"]` to get the time of the tweet and so on. \n",
    "\n",
    "Like the user profile in a `User` object, the `Status` object contains information like the text of Trump's tweet, say  retweet count and so on. Here is some basic information about Trump's most recent tweet. This is stored under index 0 in the list of tweets we fetched from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select one of the 10 tweets, here the first (most recent) one\n",
    "tweet = tweets[0]\n",
    "\n",
    "# print out some of the data associated with the \"status\" object\n",
    "# remember our \"dot\" notation for accessing data and functionality\n",
    "print(tweet.text)\n",
    "print(tweet.created_at)\n",
    "print(tweet.retweet_count)\n",
    "print(tweet.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** If we want to print out all 10 of these tweets, how would we do that? Would we do something like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tweets[0].text)\n",
    "print(tweets[1].text)\n",
    "print(tweets[2].text)\n",
    "print(tweets[3].text)\n",
    "print(tweets[4].text)\n",
    "print(tweets[5].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we needed to print out 100 tweets instead of 10? This doesn't seem like the right approach.\n",
    "\n",
    "## Introducing Loops! \n",
    "\n",
    "Loops are available in most programming languages and they simply allow code to be executed repeatedly. Let's see what this means by looking at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Introducing Loops! \n",
    "# say we have a list with three names (strings) in it\n",
    "teachers_names = ['mark', 'emily', 'mike']\n",
    "\n",
    "# we can print out the length of the list\n",
    "print(\"The list has\", len(teachers_names), \"elements\")\n",
    "\n",
    "# we can also print out the 1st, 2nd and 3rd element in the list\n",
    "print(teachers_names[0])\n",
    "print(teachers_names[1])\n",
    "print(teachers_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following is an example of a \"for\" loop\n",
    "# we will \"loop through\" the list and print out each name\n",
    "\n",
    "for name in teachers_names:\n",
    "    # start of the code to run each time we go through the loop\n",
    "    print(name)\n",
    "    # end of the code to run each time we go through the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put a `for` loop to use with some tweets. You can fetch the most recent 10 tweets from `@realDonaldTrump` and then loop over the tweets, printing each one out. Here, a tweet object from Tweepy has data attributes like the text of the tweet, stored in `\".text\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the \"real\" Donald's last 10 tweets\n",
    "tweets = api.user_timeline(screen_name='realDonaldTrump', count=10)\n",
    "\n",
    "# loop over the tweets and print out the tweet text\n",
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try This!**\n",
    "\n",
    "Use the example above to get the latest tweets for yourself, `@nytimes`, etc. What other information would be useful to have besides the text of the tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's how we pulled data about the #learntocode hashtag last week. We used the search API from Twitter, made visible via Tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = api.search(\"#learntocode\")\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Latency of AT&T Network: http://ipnetwork.bgtmo.ip.att.net/pws/network_delay.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
